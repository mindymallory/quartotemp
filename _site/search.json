[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "quarto-test-website",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "tsbootcamp/2018-01-02-Time-Series-Explainer.html",
    "href": "tsbootcamp/2018-01-02-Time-Series-Explainer.html",
    "title": "Basic Time-Series Analysis, the Game",
    "section": "",
    "text": "When graduate students approach me hoping to start research in price analysis, they usually have to start from ground zero in terms of their time-series statistical training. Even incoming graduate students well trained in econometrics often have seen little, if any, treatment of time-series techniques because first and even second econometrics courses focus most of their time on cross-sectional and panel techniques.\nFor the graduate student aspiring to do a (Ph.D.) dissertation or (M.S.) thesis in price analysis this is a real bummer because it often will be 6 to 18 months before they can work a formal time-series course into their schedule, severely delaying their research unless they can learn a lot on their own.\nThis series of posts is for my current and future new-ish graduate students who will soon start research in applied price analysis or finance, but feel overwhelmed and under-prepared in the ways of time-series econometrics (this is totally normal, by the way). I hope it will also be accessible to anyone who has had a basic class in statistics (through linear regression), and an interest in price analysis or forecasting.\nWhen you start a time-series class or read a paper that uses time-series analysis for the first time you are greeted by a barrage of new statistical tests and terms like, “Augmented Dickey-Fuller (ADF) Test”, “Johansen’s Cointegration Test”, “AR(p)”, “GARCH(p,q)”, “Vector Autoregression (VAR)” and “Vector Error Correction Model (VECM)” to name a few.\nSince a statistics or econometrics class has to cover so much… well, statistics, sometimes the forest can be lost for the trees in how all these tests and models can help one build a case around the research question they care about.\nI’m not going to get too deep into the details of these issues; for that I link to resources for how you can start to learn more on your own at the bottom. Rather, in this series I give a 30,000 ft view of ‘The Game’ that is going on in applied time-series analysis. It follows a fairly standard template."
  },
  {
    "objectID": "tsbootcamp/2018-01-02-Time-Series-Explainer.html#stationary-and-non-stationary-data-and-issues",
    "href": "tsbootcamp/2018-01-02-Time-Series-Explainer.html#stationary-and-non-stationary-data-and-issues",
    "title": "Basic Time-Series Analysis, the Game",
    "section": "Stationary and Non-Stationary Data and Issues",
    "text": "Stationary and Non-Stationary Data and Issues\nThe first thing you have to establish in any time-series analysis is whether your data are stationary or not because it essentially determines whether you should try to model levels of the data or first differences of the data.\nIf you want to see the R code that generates any of the following stuff, visit my Github repostory that has the source code for this blog post.\nStationary Data\nIn the figure below I simulate and plot a stationary series. The series comes from 500 draws of a normal distribution with mean = 300 and standard deviation = 0.25. Notice that it is basically flat, with random ups and downs.\nThat is basically what stationarity means, all the data come from a single probability distribution.\n\n# If you are following along, uncomment the next lines and run once to install the required packages \n# install.packages('ggplot2')\n# install.packages('xts')\n# install.packages(\"stargazer\")\n# install.packages('quantmod')\n# install.packages('sde')\n# install.packages('broom')\n# install.packages('tseries')\n# install.packages(\"kableExtra\")\n# install.packages(\"knitr\")\n\n\n# These lines 'load' the required packages, something you have to do every time you start an R session, even if the package is installed on your machine.  \nlibrary(ggplot2)\nlibrary(xts)\n\n# Making the fake price series\n## Dates\nend     <- Sys.Date()\nstart   <- end - 499 \nt       <- as.Date(start:end)\n\n## 500 fake prices from a normal distribution\nx       <- rnorm(n = 500, mean = 300, sd = .25)\n\n## Put dates and prices together in a time-series object\np       <- as.xts(x, order.by = t)\n\n# Plot It\nautoplot(p, ts.colour = \"dark blue\") + \n  labs(title = \"Fake Stationary Price Series Centered around P = $300\", x = \"\", y = \"Price\") + \n  theme_bw() + \n  scale_x_date(date_labels = \"%m-%d-%Y\", date_breaks = \"3 month\")\n\n\n\n\nWhen you look at this plot it is not hard to imagine that it came from draws from a single probability distribution, namely:\n\\[p_t \\sim N(\\mu = 300, \\sigma = .25)\\]\nWe need our data to be stationary in order to be able to do statistical tests and modelling on it. So if the data is not stationary, we have to transform it so it is and do our analysis on the transformed variable. The next sub-section should explain why.\nNon-Stationary or Unit Root Data\nBelow I plot SPY, the exchange traded fund that tracks the S&P 500 Index, from 1990 to 2017. By visual inspection you should have a sense that this series is non-stationary in that the prices appear to be trending.\n\nlibrary(quantmod)\ngetSymbols(c('SPY', 'GS'))\n\n[1] \"SPY\" \"GS\" \n\n\n\n# In Yahoo Finance data, the Adjusted column accounts for any stock splits that may have taken place.\nautoplot(SPY$SPY.Adjusted) + \n  theme_bw() + \n  geom_line(color = \"dark blue\") + \n  labs(title = \"SPY Prices from 2007 to 2017\", x = \"\")\n\n\n\n\nSo for example, if you wanted to write down the probability distribution that generate the prices, you would have to know where you are in time. Since the probability distribution that generated the prices keeps changing over time, the distribution is not stationary. This fact will mess up our statistical analysis.\n\\[p_t \\sim N(\\mu_t, \\sigma_t)\\]\nSpurious Regression\nOne of the worst things non-stationary data does is set you up to believe that you have found a really strong relationship between two variables, when you really have just found a ‘spurious relationship’. In the figure below I plot SPY and Goldman Sachs (GS) prices from 2007 to 2017.\n\n\n\n\n\n\n\n\n\nTo illustrate a ‘spurious regression’ let’s regress GS prices on lagged SPY prices. That means we are going to try to use yesterday’s prices of SPY to try to predict today’s GS prices.\n\\[GS_t = \\beta_0 + \\beta_1SPY_{t-1}  + \\epsilon_t\\] The results of the regression are printed below. We find that lagged SPY prices are highly statistically significant predictors of GS prices. If you take these results to the stock market and think you can predict GS prices you will probably be very poor very soon.\nThe problem is that the ‘statistical significance’ comes from the fact that we just ran a super-consistent regression. That basically means that since the variables we used in the regression are (probably, we didn’t prove it here) non-stationary pretty much any regression we run will conclude statistical significance in a typical t-test. In other words, the regression results below aren’t valid because the statistic doesn’t follow the t distribution in the first place, but when you go along as it it does follow the t distribution, you will get ‘significant’ results almost every time.\n\nlibrary(stargazer)\nGS <- GS$GS.Adjusted\nSPY <- SPY$SPY.Adjusted\nlinMod <- lm(GS ~  lag(SPY))\nstargazer(linMod, type  = \"html\", title = \"GS Prices Regressed on Lagged GS and Lagged SPY Prices\", align = TRUE)\n\n\n\nGS Prices Regressed on Lagged GS and Lagged SPY Prices\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nGS\n\n\n\n\n\n\n\n\nlag(SPY)\n\n\n0.642***\n\n\n\n\n\n\n(0.004)\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n51.244***\n\n\n\n\n\n\n(1.004)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n4,066\n\n\n\n\nR2\n\n\n0.836\n\n\n\n\nAdjusted R2\n\n\n0.836\n\n\n\n\nResidual Std. Error\n\n\n30.854 (df = 4064)\n\n\n\n\nF Statistic\n\n\n20,746.460*** (df = 1; 4064)\n\n\n\n\n\n\n\n\nNote:\n\n\np<0.1; p<0.05; p<0.01\n\n\n\n\n\nYou may be skeptical at this point. I mean, after all, it isn’t that crazy to think that today’s GS prices would be related to yesterday’s SPY prices. So to illustrate, now we’ll simulate two series so we are sure they are not related. Then we will regress one on the lag of the other and show statistical significance.\nThe simulated prices are plotted below.\n\n# The function, GBM() from the sde package simulates a geometric brownian motion. Which is the same a assuming prices follow a lognormal ditribution, and is the standard baseline model for prices. \nlibrary(sde)\nfakeSPY   <- GBM(x = 275, r = 0.15, sigma = .2, T = 1, N=650)\nfakeGS    <- GBM(x = 243, r = 0.10, sigma = .3, T = 1, N=650)\n\n\nlibrary(broom)\ndates     <- seq(as.Date(\"2018-01-01\"), length = 651, by = \"days\")\nfakeSPY   <- xts(x=fakeSPY, order.by = dates)\nfakeGS    <- xts(x=fakeGS, order.by = dates)\n\ndata      <- cbind(fakeSPY, fakeGS)\ncolnames(data) <- c('fakeSPY', 'fakeGS')\ndata      <- tidy(data)\n\nggplot(data, aes(x = index, y = value, color = series)) + \n  geom_line() + \n  theme_bw() +\n  labs(title = \"Simulated SPY and Simulated GS Prices from 2007 to 2017\", x = \"\")\n\n\n\n\nNow lets run the same regression as we did with the real prices. The regression results are shown in the table.\n\\[fakeGS_t = \\beta_0 + \\beta_1fakeSPY_{t-1}  + \\epsilon_t\\]\n\nlinMod <- lm(fakeGS ~  lag(fakeSPY) )\nstargazer(linMod, type  = \"html\", title = \"Simulated GS Prices Regressed on Lagged Simulated SPY Prices\")\n\n\n\nSimulated GS Prices Regressed on Lagged Simulated SPY Prices\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nfakeGS\n\n\n\n\n\n\n\n\nlag(fakeSPY)\n\n\n1.946***\n\n\n\n\n\n\n(0.162)\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n-202.504***\n\n\n\n\n\n\n(40.478)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n650\n\n\n\n\nR2\n\n\n0.183\n\n\n\n\nAdjusted R2\n\n\n0.181\n\n\n\n\nResidual Std. Error\n\n\n53.860 (df = 648)\n\n\n\n\nF Statistic\n\n\n144.747*** (df = 1; 648)\n\n\n\n\n\n\n\n\nNote:\n\n\np<0.1; p<0.05; p<0.01\n\n\n\n\n\nHuh, still crazy significant. It’s not a real relationship, but appears to be significant because we didn’t have any business doing a t-test here in the first place. We know the regression is meaningless because we simulated these prices to be independent ourselves. That is the problem with non-stationary data in a nutshell, if you regress non-stationary variables on one another, you will always get spurious ‘significant’ results.\nSo what to do? The answer is almost always this: if your data are non-stationary, convert the series into percentage changes by creating variables of logged price differences. In the context of the examples we have been using, that would be\n\\[SPYReturn_t = log(SPY_t) - log(SPY_{t-1}).  \\] Plotted, the price returns look like the following.\n\nSPYRet    <- log(SPY) - log(lag(SPY))\nGSRet     <- log(GS) - log(lag(GS))\ndata      <- cbind(SPYRet, GSRet)\n\ncolnames(data) <- c('SPY', 'GS')\ndata      <- tidy(data)\n\nggplot(data, aes(x = index, y = value, color = series)) + \n  geom_line() + \n  theme_bw() +\n  labs(title = \"SPY Returns and GS Returns from 2007 to 2017\", x = \"\")\n\n\n\n\nClearly, this is not exactly white noise. We have clear periods of heightened volatility (we’ll cover what to do about that in another post). But, at least the series are not trending up or down. We have solved the problem of non-stationarity by using price percent returns instead of price levels. Just to drive this point home, let’s regress GS returns on lagged SPY returns and show they are no longer significant.\n\nlinMod <- lm(GSRet ~  lag(SPYRet) )\nstargazer(linMod, type  = \"html\", title = \"GS Returns Regressed on Lagged SPY Returns\")\n\n\n\nGS Returns Regressed on Lagged SPY Returns\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nGSRet\n\n\n\n\n\n\n\n\nlag(SPYRet)\n\n\n-0.035\n\n\n\n\n\n\n(0.028)\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n0.0002\n\n\n\n\n\n\n(0.0004)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n4,065\n\n\n\n\nR2\n\n\n0.0004\n\n\n\n\nAdjusted R2\n\n\n0.0001\n\n\n\n\nResidual Std. Error\n\n\n0.023 (df = 4063)\n\n\n\n\nF Statistic\n\n\n1.508 (df = 1; 4063)\n\n\n\n\n\n\n\n\nNote:\n\n\np<0.1; p<0.05; p<0.01\n\n\n\n\n\nThe statistical significance is gone! Bummer! But really this is good. It is a sign that the returns we generated are in fact stationary."
  },
  {
    "objectID": "tsbootcamp/2018-01-24-Basic-Time-Series-Analysis-Does-Stuff-Move-Together.html",
    "href": "tsbootcamp/2018-01-24-Basic-Time-Series-Analysis-Does-Stuff-Move-Together.html",
    "title": "Basic Time-Series Analysis: The VAR Model Explained?",
    "section": "",
    "text": "This post is the third in a series explaining Basic Time Series Analysis. Click the link to check out the first post which focused on stationarity versus non-stationarity, and to find a list of other topics covered. As a reminder, this post is intended to be a very applied example of how use certain tests and models in a time-sereis analysis, either to get someone started learning about time-series techniques or to provide a big-picture perspective to someone taking a formal time-series class where the stats are coming fast and furious. As in the first post, the code producing these examples is provided for those who want to follow along in R. If you aren’t into R, just ignore the code blocks and the intuition will follow.\nIn this post I explain how one goes about estimating the relationship among several variables over time. This approach has natural applications in agricultural economics and finance. In ag econ there are commodities whose prices are inherently related because of substitution or complementary effects in production and/or consumption (e.g., corn and soybeans), or because of production processes (e.g., soybeans, soybean oil, and soybean meal). In finance, security prices for companies in a similar sector might be related because of common economic conditions driving profitability (e.g., Bank of America and J.P. Morgan Chase).\nIn time-series analysis, there are two basic models typically used to estimate and evaluate the relationships between multiple variables over time.\nWe will start with the Vector Auto-regression model, because it is the simpler one. The next post will cover VECM which estimates how a group of variables move together in equilibrium. Also for simplicity, we will continue as in the first post using SPY (the S&P 500 exchange traded fund) and GS (Goldman Sachs) prices."
  },
  {
    "objectID": "tsbootcamp/2018-01-24-Basic-Time-Series-Analysis-Does-Stuff-Move-Together.html#levels-or-returns",
    "href": "tsbootcamp/2018-01-24-Basic-Time-Series-Analysis-Does-Stuff-Move-Together.html#levels-or-returns",
    "title": "Basic Time-Series Analysis: The VAR Model Explained?",
    "section": "Levels or Returns?",
    "text": "Levels or Returns?\nIf you read my first post in the series you should be wondering why in the world it is OK to put the SPY and GS prices into the VAR in levels. After all, we found strong evidence that both are non-stationary. This is true, the VAR model written above would have all the problems of a spurious regression we discussed in the first post. Writing the VAR using returns instead of price levels will usually remedy the situation, as noted in the first post.\nThen, a VAR(2) using price returns (using the \\(\\Delta\\) notation to indicate \\(\\Delta SPY_t = log(SPY_t) - log(SPY_{t-1})\\)) is\n\\[\\begin{align}\n\\Delta SPY_t &= \\beta^{spy}_0 + \\beta^{spy}_1 \\Delta SPY_{t-1} + \\beta^{spy}_2 \\Delta SPY_{t-2} + \\beta^{spy}_3 \\Delta GS_{t-1} + \\beta^{spy}_4 \\Delta GS_{t-2} + \\epsilon_{spy} \\\\\n\\Delta GS_t  &= \\beta^{gs}_0  + \\beta^{gs}_1 \\Delta SPY_{t-1}  + \\beta^{gs}_2 \\Delta SPY_{t-2}  + \\beta^{gs}_3 \\Delta GS_{t-1}  + \\beta^{gs}_4 \\Delta GS_{t-2}  + \\epsilon_{gs}\n\\end{align}\\]\nFitting a VAR with two lags to SPY and GS returns yields the following."
  },
  {
    "objectID": "tsbootcamp/2018-01-24-Basic-Time-Series-Analysis-Does-Stuff-Move-Together.html#var2-on-spy-and-gs-returns",
    "href": "tsbootcamp/2018-01-24-Basic-Time-Series-Analysis-Does-Stuff-Move-Together.html#var2-on-spy-and-gs-returns",
    "title": "Basic Time-Series Analysis: The VAR Model Explained?",
    "section": "VAR(2) on SPY and GS Returns",
    "text": "VAR(2) on SPY and GS Returns\n\nlibrary(vars)\nlibrary(stargazer)\n\nvar <- VAR(time_series[2:dim(time_series)[1],], p = 2, type = \"const\", )\nstargazer(var$varresult$SPY, var$varresult$GS, type = 'html', dep.var.labels = c(\"Equation 1-SPY  Equation 2-GS\"))\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nEquation 1-SPY Equation 2-GS\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nSPY.l1\n\n\n-0.052**\n\n\n0.094**\n\n\n\n\n\n\n(0.023)\n\n\n(0.042)\n\n\n\n\n\n\n\n\n\n\n\n\nGS.l1\n\n\n-0.043***\n\n\n-0.096***\n\n\n\n\n\n\n(0.013)\n\n\n(0.023)\n\n\n\n\n\n\n\n\n\n\n\n\nSPY.l2\n\n\n-0.059**\n\n\n-0.063\n\n\n\n\n\n\n(0.023)\n\n\n(0.042)\n\n\n\n\n\n\n\n\n\n\n\n\nGS.l2\n\n\n0.029**\n\n\n0.035\n\n\n\n\n\n\n(0.013)\n\n\n(0.023)\n\n\n\n\n\n\n\n\n\n\n\n\nconst\n\n\n0.0004*\n\n\n0.0002\n\n\n\n\n\n\n(0.0002)\n\n\n(0.0004)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n4,064\n\n\n4,064\n\n\n\n\nR2\n\n\n0.017\n\n\n0.005\n\n\n\n\nAdjusted R2\n\n\n0.016\n\n\n0.004\n\n\n\n\nResidual Std. Error (df = 4059)\n\n\n0.013\n\n\n0.023\n\n\n\n\nF Statistic (df = 4; 4059)\n\n\n17.034***\n\n\n5.531***\n\n\n\n\n\n\n\n\nNote:\n\n\np<0.1; p<0.05; p<0.01\n\n\n\n\n\nIn the SPY returns equation (1), the first lag of GS returns and the second lag of SPY returns are statistically significant. Also, in the GS returns equation (2), the first and second lag of SPY returns are statistically significant, and the first lag of GS returns is statistically significant."
  },
  {
    "objectID": "tsbootcamp/2018-01-24-Basic-Time-Series-Analysis-Does-Stuff-Move-Together.html#what-is-it-used-for",
    "href": "tsbootcamp/2018-01-24-Basic-Time-Series-Analysis-Does-Stuff-Move-Together.html#what-is-it-used-for",
    "title": "Basic Time-Series Analysis: The VAR Model Explained?",
    "section": "What is it Used for?",
    "text": "What is it Used for?\nThe VAR model is used to determine the relationship among several variables. You can use a VAR for forecasting, like we did with the ARIMA and GARCH models, but as we found with those, the forecasts are usually not precise enough to be all that informative from a practical standpoint. Instead, in practice the researcher will usually end up looking at the following three things that are derived from the fitted VAR model: Granger Causality, Impulse Response Functions, and Forecast Error Variance Decomposition, that reveal something about the nature of how these markets move together (or not).\nGranger Causality is most commonly implemented by an F-test on the lags of the other variable on the variables of interest. Stated more simply in our context, it tests whether lags of SPY returns are helpful in forecasting GS returns, and vice versa. Impulse response functions show how one variable might react to sudden changes in the other variable. Finally, forecast error variance decomposition (FEVD) estimates how much of your forecast error can be attributed to unpredictability in each variable in the VAR.\nWe will look more closely at each."
  },
  {
    "objectID": "tsbootcamp/2018-01-24-Basic-Time-Series-Analysis-Does-Stuff-Move-Together.html#granger-causality",
    "href": "tsbootcamp/2018-01-24-Basic-Time-Series-Analysis-Does-Stuff-Move-Together.html#granger-causality",
    "title": "Basic Time-Series Analysis: The VAR Model Explained?",
    "section": "Granger Causality",
    "text": "Granger Causality\nGranger Causality is a different kind of causality than one typically runs into in cross-section econometrics, where you might have some kind of natural experiment. A typical story-line of that type might be something like the following: an unpredictable policy change gave a random subset of people more access to credit. An econometrician might then come along and test if greater access to credit leads to X, Y, or Z - like college enrollment, or mechanization on small-holder farms, etc. depending on the context. In that case, the econometrician will try to convince you whether or not access to credit really caused the change in outcome. Then, once causality has been established (or not), policy prescriptions might be suggested, perhaps to encourage (or not) the provision of credit.\nIn time-series econometrics, we can seldom hope to show ‘real’ causality. We settle for Granger causality. Are these things correlated enough that one is useful in forecasting the other? If so, Granger Causality can be established.\nThe following code takes the VAR regression output object var, and tests for Granger causality of SPY returns on GS returns and vice versa. The F-test is done the standard way in the lines where boot = FALSE, and using bootstrapped standard errors in the lines where boot = TRUE.1 Since we already looked in detail and found heteroskedasticity in these return series in the previous GARCH post, we should be concerned that the standard errors of the F-test calculated in the usual way are too narrow. Note that in the output table below the code that the F-statistics are the same using both methods, but the P-values are bigger using the bootstrap. This is because the bootstrap increases the standard errors according to how bad the heteroskedasticity is."
  },
  {
    "objectID": "tsbootcamp/2018-01-24-Basic-Time-Series-Analysis-Does-Stuff-Move-Together.html#tests-for-granger-causality",
    "href": "tsbootcamp/2018-01-24-Basic-Time-Series-Analysis-Does-Stuff-Move-Together.html#tests-for-granger-causality",
    "title": "Basic Time-Series Analysis: The VAR Model Explained?",
    "section": "Tests for Granger Causality",
    "text": "Tests for Granger Causality\n\nlibrary(broom)\nlibrary(knitr)\nlibrary(kableExtra)\ncauseSPY_noboot <- causality(var, cause = 'SPY', boot = FALSE, boot.runs = 5000)$Granger %>% tidy()\ncauseSPY_boot   <- causality(var, cause = 'SPY', boot = TRUE, boot.runs = 5000)$Granger %>% tidy()\ncauseGS_noboot  <- causality(var, cause = 'GS', boot = FALSE, boot.runs = 5000)$Granger %>% tidy()\ncauseGS_boot    <- causality(var, cause = 'GS', boot = TRUE, boot.runs = 5000)$Granger %>% tidy()\n\ncauseSPY_noboot$parameter <- NA\ncauseGS_noboot$parameter  <- NA\ncauseSPY_boot$df1         <- NA\ncauseSPY_boot$df2         <- NA\ncauseGS_boot$df1         <- NA\ncauseGS_boot$df2         <- NA\n\n\nresultstable   <- rbind(causeGS_noboot[, c(1, 2, 6, 3, 4, 5)], causeSPY_noboot[, c(1, 2, 6, 3, 4, 5)], causeGS_boot[, c(5, 6, 3, 1, 2, 4)], causeSPY_boot[, c(5, 6, 3, 1, 2, 4)])\n\nresultstable$Bootstrap    <- c('No Bootstrap', '', 'Bootstrap', '')\nresultstable   <- resultstable[, c(7, 1, 2, 3, 4, 5, 6)]\n\ncolnames(resultstable)    <- c('Bootstrap', 'df1', 'df2', '#Bootstraps', 'Stat', 'P-Value', 'Equation')\n  \nresultstable %>% \n  kable(\"html\") %>% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n \n  \n    Bootstrap \n    df1 \n    df2 \n    #Bootstraps \n    Stat \n    P-Value \n    Equation \n  \n \n\n  \n    No Bootstrap \n    2 \n    8118 \n    NA \n    8.432894 \n    0.0002195 \n    Granger causality H0: GS do not Granger-cause SPY \n  \n  \n     \n    2 \n    8118 \n    NA \n    3.894321 \n    0.0203952 \n    Granger causality H0: SPY do not Granger-cause GS \n  \n  \n    Bootstrap \n    NA \n    NA \n    5000 \n    8.432894 \n    0.0804000 \n    Granger causality H0: GS do not Granger-cause SPY \n  \n  \n     \n    NA \n    NA \n    5000 \n    3.894321 \n    0.3928000 \n    Granger causality H0: SPY do not Granger-cause GS \n  \n\n\n\n\n\nWithout the bootstrapped standard errors we would conclude strongly that SPY Granger causes GS and GS Granger causes SPY. When we use bootstrapped standard errors we fail to reject at a 5% confidence level both that GS returns do not Granger cause SPY returns and that SPY returns do not Granger cause GS returns (so we conclude they do not, but just barely). Or, if you like, you could say that at the 10% confidence level, both SPY returns Granger cause GS returns and GS returns Granger cause SPY returns. Pick you narrative."
  },
  {
    "objectID": "tsbootcamp/2018-01-24-Basic-Time-Series-Analysis-Does-Stuff-Move-Together.html#forecast-error-variance-decomposition",
    "href": "tsbootcamp/2018-01-24-Basic-Time-Series-Analysis-Does-Stuff-Move-Together.html#forecast-error-variance-decomposition",
    "title": "Basic Time-Series Analysis: The VAR Model Explained?",
    "section": "Forecast Error Variance Decomposition",
    "text": "Forecast Error Variance Decomposition\nA Forecast Error Variance Decomposition is another way to evaluate how markets affect each other using the VAR model. In a FEVD, forecast errors are considered for each equation in the fitted VAR model, then the fitted VAR model is used to determine how much of each error realization is coming from unexpected changes (forecast errors) in the other variable. We can calculate this conveniently with fevd() from the vars package.\n\nFEVD <- fevd(var, n.ahead = 5)\nplot(FEVD)\n\n\n\n\nIn the first plot, we see the FEVD for SPY. It appears that although we were borderline on whether or not to conclude that GS returns Granger cause SPY returns, the FEVD reveals that the magnitude of the causality is tiny anyway (we can’t even see the contribution from GS returns on the FEVD graph). Conversely, it appears SPY returns account for about half of the forecast error variance in the GS equation. That seems economically significant to me."
  },
  {
    "objectID": "tsbootcamp/2018-01-24-Basic-Time-Series-Analysis-Does-Stuff-Move-Together.html#impulse-response-function",
    "href": "tsbootcamp/2018-01-24-Basic-Time-Series-Analysis-Does-Stuff-Move-Together.html#impulse-response-function",
    "title": "Basic Time-Series Analysis: The VAR Model Explained?",
    "section": "Impulse Response Function",
    "text": "Impulse Response Function\nImpluse Response Functions have a similar motivation, but go about it in a little bit different way. In this exercise, you take a shock to one variable, say SPY, and propagate it through the fitted VAR model for a number of periods. You can trace this through the VAR model and see if it impacts the other variables in a statistically significant way. Also from the vars package, this is easily achieved with the irf() function. Here also confidence intervals are produced by bootstrapping.\n\nIRF <- irf(var, impulse = 'SPY', response = 'GS',  n.ahead = 5, boot = TRUE, runs = 100, ci = 0.95)\nplot(IRF)\n\n\n\nIRF <- irf(var, impulse = 'GS', response = 'SPY',  n.ahead = 5, boot = TRUE, runs = 100, ci = 0.95)\nplot(IRF)\n\n\n\n\nIt appears that a one standard deviation to SPY returns produces a statistically significant response to GS returns for one period, then becomes insignificant. The one standard deviation to GS returns produces a statistically significant response to SPY two periods later (also nearly statistically significant three periods later). Eyeballing the size of these effects, it looks to me like the FEVD and impulse response analysis point to similar findings. The impact of SPY returns on GS returns appears to be sizable in the IRF, while the the impact of GS returns seems to be minimal on SPY returns from an economic impact perspective."
  },
  {
    "objectID": "tsbootcamp/2018-01-25-Basic-Time-Series-Analysis-ARIMA-GARCH.html",
    "href": "tsbootcamp/2018-01-25-Basic-Time-Series-Analysis-ARIMA-GARCH.html",
    "title": "Basic Time-Series Analysis: Single Equation Models (ARIMA)",
    "section": "",
    "text": "This post is the second in a series explaining Basic Time Series Analysis. Click the link to check out the first post which focused on stationarity versus non-stationarity, and to find a list of other topics covered. As a reminder, this post is intended to be a very applied example of how use certain tests and models in a time-sereis analysis, either to get someone started learning about time-series techniques or to provide a big-picture perspective to someone taking a formal time-series class where the stats are coming fast and furious. As in the first post, the code producing these examples is provided for those who want to follow along in R. If you aren’t into R, just ignore the code blocks and the intuition will follow.\nIn this post we cover the ARIMA model for a single time series. Single equation models like the ARIMA are generally used more in a forecasting context rather than in an economic analysis context wherein uncovering insights into, or relationships between, markets might be the goal. Single market models, which look at just one series of prices at a time, can only reveal so much economic insight. For determining relationships among many markets, models like the VAR and VECM are much more useful than the ARIMA; soon we’ll cover those models in a future post. However, even if you don’t care about forecasting and you only want to study market linkages, you will have an easier time understanding where the VAR and VECM models come from if you have been exposed to the ARIMA model first.\nMake no mistake though, forecasting is a valid pursuit in its own right. Single equation models basically try to infer from historical patterns in the price or return series what the likely next realization will be, given recent history. Admittedly, these models do not give very precise predictions, but the confidence intervals they produce are informative as to what kind of range should be expected of future values.\nFor simplicity, we will continue as in the first post using SPY (the S&P 500 exchange traded fund) prices to illustrate.1\nThe code below pulls SPY prices from Yahoo Finance, converts them to percent returns, and plots it. In the first post we already determined that SPY has a unit root, so we already know we should generally be working with the data as percent returns.\nNow think about this series in the same context of trying to write down the probability distribution that generate the returns. In the first post of this series, we noted that a proper statistical model should express returns as coming from a single stationary probability distribution.\n\\[r^{SPY}_t \\sim N(\\mu, \\sigma^2)\\]\nAlso in the first post we argued that differencing a non-stationary series usually gets you pretty far toward this goal. This post will continue trying to improve on this objective. Note first that we have not quite achieved a series that follows a single probability distribution (iid is the stats jargon for this) simply by differencing. It is quite obvious that the volatility of this series changes over time. To correct for changing variance or volatility General Auto-regressive Conditional Heteroskedasticity (GARCH) models can be used.\nLess obvious is that the mean of this series is or might be changing over time. From the plot, you might be able to detect short runs of positive or negative returns. Put another way, although the overall mean looks pretty constant at something near zero, if you focused on a short span of this data, it could have positive or negative returns. Put yet another way, returns could be correlated over time. To correct for a changing mean, or correlation over time in the returns, an ARIMA model can be used.\nWe’ll discuss the ARIMA model in detail below and the GARCH model in the next post, but remember this: We know returns are not iid (iid \\(\\rightarrow\\) generated by a single probability distribution).\n\\[r^{SPY}_t \\sim N(\\mu_t, \\sigma_t^2)\\]\nThe ARIMA model is easiest understood by breaking it down into it’s parts."
  },
  {
    "objectID": "tsbootcamp/2018-01-25-Basic-Time-Series-Analysis-ARIMA-GARCH.html#acfh",
    "href": "tsbootcamp/2018-01-25-Basic-Time-Series-Analysis-ARIMA-GARCH.html#acfh",
    "title": "Basic Time-Series Analysis: Single Equation Models (ARIMA)",
    "section": "ACF(h)",
    "text": "ACF(h)\nThe auto-correlation function of h, ACF(h), is the correlation between \\(r^{SPY}_t\\) and \\(r^{SPY}_{t-h}\\). So ACF(1) is \\(Corr(r^{SPY}_t, r^{SPY}_{t-1})\\), ACF(2) is \\(Corr(r^{SPY}_t, r^{SPY}_{t-2})\\), etc.\nIn R, the Acf() function from the forecast package computes these for as many lags (h’s) as you want and plots them. It’s a quick way to know if your data have auto-correlation problems to be fixed.\n\nlibrary(forecast)\nggAcf(SPYRet_xts, lag.max = 10) + theme_bw()\n\n\n\n\nThe horzontal blue lines on this chart are the significance lines. So the spikes above or below the blue lines indicate significant auto-correlation of current returns with that lag of returns. So it looks like there is indeed auto-correlation to account for in SPY returns. The lines for the first, second, and fifth lags spike above the blue lines."
  },
  {
    "objectID": "tsbootcamp/2018-01-25-Basic-Time-Series-Analysis-ARIMA-GARCH.html#pacf",
    "href": "tsbootcamp/2018-01-25-Basic-Time-Series-Analysis-ARIMA-GARCH.html#pacf",
    "title": "Basic Time-Series Analysis: Single Equation Models (ARIMA)",
    "section": "PACF",
    "text": "PACF\nThe partial auto-correlation function tells you how much correlation there is between current returns and a lagged return after removing the correlation from all the previous lags. Its kind of like isolating which lags are really driving auto-correlation and which are just propagating the effects through the system.\nThere is a similar function in R to calculate the PACF() function.\n\nggPacf(SPYRet_xts, lag.max = 10) + theme_bw()\n\n\n\n\nSince this function is more focused on isolating which lags are important it can be used to inform how many lagged returns should be included in a model to explain returns. In this case you would definitely include at least two, and possibly up to five lags. The fifth lag is a judgement call because at some level it is hard to believe returns from five days ago will be useful in predicting or explaining current returns. And, you give up degrees of freedom and power the more regressors you put in the model. Although in this particular case we have a lot of data in our series, it wouldn’t matter too much."
  },
  {
    "objectID": "tsbootcamp/2018-01-25-Basic-Time-Series-Analysis-ARIMA-GARCH.html#the-arp-model",
    "href": "tsbootcamp/2018-01-25-Basic-Time-Series-Analysis-ARIMA-GARCH.html#the-arp-model",
    "title": "Basic Time-Series Analysis: Single Equation Models (ARIMA)",
    "section": "The AR(p) Model",
    "text": "The AR(p) Model\nAn auto-regressive model of order p, AR(p) just means that we are going to try to explain SPY returns with p lags of of SPY returns. An AR(5) model for SPY returns looks like the following:\n\\[\\begin{align}\nr^{SPY}_t &= \\beta_0 + \\beta_1r^{SPY}_{t-1} + \\beta_2r^{SPY}_{t-2} +  \\beta_3r^{SPY}_{t-3} + \\beta_4r^{SPY}_{t-4} + \\beta_5r^{SPY}_{t-5} + \\epsilon_t \\\\\n\\end{align}\\]\nThe following code generates lags of the returns to put on the right hand side of the regression equation, and the last line does the estimation.\n\nSPYRet_1   <- lag(SPYRet_xts)\nSPYRet_2   <- lag(SPYRet_1)\nSPYRet_3   <- lag(SPYRet_2)\nSPYRet_4   <- lag(SPYRet_3)\nSPYRet_5   <- lag(SPYRet_4)\nAR2           <- lm(SPYRet_xts ~  SPYRet_1 + SPYRet_2 + SPYRet_3 + SPYRet_4 + SPYRet_5)\n\n\nEstimated AR(5) Model of SPY Returns\nlibrary(kableExtra)\nlibrary(knitr)\nlibrary(tibble)\nlibrary(broom)\n\nglance(AR2)%>% \n  kable(\"html\") %>% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n \n  \n    r.squared \n    adj.r.squared \n    sigma \n    statistic \n    p.value \n    df \n    logLik \n    AIC \n    BIC \n    deviance \n    df.residual \n    nobs \n  \n \n\n  \n    0.0137702 \n    0.0125541 \n    0.0128185 \n    11.32354 \n    0 \n    5 \n    11933.91 \n    -23853.83 \n    -23809.66 \n    0.6662966 \n    4055 \n    4061 \n  \n\n\n\n\nAR2coef           <- tidy(AR2, stringsAsFactors = FALSE) \nAR2coef           <- cbind(AR2coef[, 1], round(AR2coef[, 2:5], digits = 2))\ncolnames(AR2coef) <- c('Variable', 'Beta Estimate', 'Std. Error', 't-statistic', 'P-Value')\n\nAR2coef %>% \n  kable(\"html\") %>% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n \n  \n    Variable \n    Beta Estimate \n    Std. Error \n    t-statistic \n    P-Value \n  \n \n\n  \n    (Intercept) \n    0.00 \n    0.00 \n    1.93 \n    0.05 \n  \n  \n    SPYRet_1 \n    -0.11 \n    0.02 \n    -7.08 \n    0.00 \n  \n  \n    SPYRet_2 \n    -0.02 \n    0.02 \n    -1.50 \n    0.13 \n  \n  \n    SPYRet_3 \n    0.01 \n    0.02 \n    0.68 \n    0.50 \n  \n  \n    SPYRet_4 \n    -0.03 \n    0.02 \n    -1.97 \n    0.05 \n  \n  \n    SPYRet_5 \n    -0.02 \n    0.02 \n    -1.15 \n    0.25 \n  \n\n\n\n\nThe coefficient estimate for the intercept term is zero to two decimal places and not significant. So on average, SPY returns are zero. This is in line with what we eyeballed from the figure of plotted returns. However, we do find that the first, second, and fifth lags of SPY returns are statistically significant, with P-Values less than 0.01 in this case. This is what we would have expected from the PACF. It is not a very economically significant result however. Notice that the R^2 is super small, meaning that we are not explaining much of the variation, and we didn’t get much forecasting power out of the model.\nIf we put confidence intervals around the forecast, it will definitely include zero return, which means if we were trying to use this forecast to inform trading decisions, we wouldn’t even know whether to buy or sell.\nBut I digress…"
  },
  {
    "objectID": "tsbootcamp/2018-01-25-Basic-Time-Series-Analysis-ARIMA-GARCH.html#estimated-arima2-1-1-model-of-spy-prices",
    "href": "tsbootcamp/2018-01-25-Basic-Time-Series-Analysis-ARIMA-GARCH.html#estimated-arima2-1-1-model-of-spy-prices",
    "title": "Basic Time-Series Analysis: Single Equation Models (ARIMA)",
    "section": "Estimated ARIMA(2, 1, 1) Model of SPY Prices",
    "text": "Estimated ARIMA(2, 1, 1) Model of SPY Prices\n\nlibrary(sweep)\n# So earlier when we fitted the MA(5) model we were using the ARIMA function, but forcing the AR terms to have zero lags. \n# We set d = 0 here becuase we already created returns, thus already differenced. If we put price levels into this function, you would want to sed d = 1. \n\nARIMA <- Arima(SPYRet_xts, order = c(2, 0, 1))\n\nglance(ARIMA)%>% \n  kable(\"html\") %>% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n \n  \n    sigma \n    logLik \n    AIC \n    BIC \n    nobs \n  \n \n\n  \n    0.0128174 \n    11947.96 \n    -23885.91 \n    -23854.36 \n    4066 \n  \n\n\n\n\nARIMAcoef           <- tidy(ARIMA) \ntstat            <- ARIMAcoef[, 2]/ARIMAcoef[ , 3]\ndf               <- length(SPYRet_xts) -  3 \npvalue           <- abs(tstat$estimate) %>% dt(df) \nARIMAcoef           <- cbind(ARIMAcoef[2:3], tstat, pvalue) %>% round(digits = 2) %>% cbind(ARIMAcoef[,1])\nARIMAcoef            <- ARIMAcoef[, c(5, 1, 2, 3, 4)]\ncolnames(ARIMAcoef) <- c('Variable', 'Beta Estimate', 'Std. Error', 't-statistic', 'P-Value')\n\nARIMAcoef %>% \n  kable(\"html\") %>% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n \n  \n    Variable \n    Beta Estimate \n    Std. Error \n    t-statistic \n    P-Value \n  \n \n\n  \n    ar1 \n    -0.06 \n    NaN \n    NaN \n    NaN \n  \n  \n    ar2 \n    -0.02 \n    NaN \n    NaN \n    NaN \n  \n  \n    ma1 \n    -0.06 \n    NaN \n    NaN \n    NaN \n  \n  \n    intercept \n    0.00 \n    0 \n    1.86 \n    0.07 \n  \n\n\n\n\n\nIt looks like we didn’t gain a lot by moving from the AR(5) or MA(5) to the ARIMA model. In our estimated ARIMA model only the second lag of the AR part is significant. Oh, well!"
  },
  {
    "objectID": "tsbootcamp/2018-01-28-Basic-Time-Series-GARCH.html",
    "href": "tsbootcamp/2018-01-28-Basic-Time-Series-GARCH.html",
    "title": "Basic Time-Series Analysis: Modeling Volatility (GARCH)",
    "section": "",
    "text": "This post is the third in a series explaining Basic Time Series Analysis. Click the link to check out the first post which focused on stationarity versus non-stationarity, and to find a list of other topics covered. As a reminder, this post is intended to be a very applied example of how use certain tests and models in time-sereis analysis, either to get someone started learning about time-series techniques or to provide a big-picture perspective to someone taking a formal time-series class where the stats are coming fast and furious. As in the first post, the code producing these examples is provided for those who want to follow along in R. If you aren’t into R, just ignore the code blocks and the intuition will follow.\nIn this post we will learn a standard technique for modelling volatility in a series of prices, the generalized auto-regressive conditional heteroskedasticity (GARCH). We build on the previous post, Basic Time-Series Analysis, Single Equation Models (ARIMA), where we learned the useful techniques of using recent returns (AR) and residuals (MA) to explain price returns.\nThe idea of the GARCH model of price volatility is to use recent realizations of the error structure to predict future realizations of the error structure. Put more simply, we often see clustering in periods of high or low volatility, so we can exploit the recent volatility to predict volatility in the near future.\nContinuing the examples in previous posts, we will use SPY prices to illustrate volatility modeling. The plot below shows SPY price returns from 2007 through 2017.\n\n# If you are following along in R, uncomment the next lines and run once to install the required packages \n# install.packages('ggplot2')\n# install.packages('xts')\n# install.packages('quantmod')\n# install.packages('broom')\n# install.packages('rugarch')\n# install.packages('tibble')\nlibrary(quantmod)\nlibrary(ggplot2)\nlibrary(broom)\ngetSymbols(c('SPY'))\n\n[1] \"SPY\"\n\nSPY              <- SPY$SPY.Adjusted\nSPYRet           <- log(SPY) - log(lag(SPY))\nSPYRet_xts       <- SPYRet\ncolnames(SPYRet) <- c('SPY')\nSPYRet           <- tidy(SPYRet)\n\nggplot(SPYRet, aes(x = index, y = value, color = series)) + \n  geom_line() + \n  theme_bw() +\n  labs(title = \"SPY Returns Returns from 2007 to 2017\", x = \"\")\n\n\n\n\nAs a reminder, the over-arching goal of of this and the previous posts has been to model the changing mean and variance of the price return series.\n\\[r^{SPY}_t \\sim N(\\mu_t, \\sigma_t^2)\\] The previous post used the ARIMA model to give structure to the changing mean of the series of price returns. Since the ARIMA model assumed constant variance, and the figure of SPY returns clearly has changing variance over time, this is something that can be improved upon, and the GARCH model is one way of accomplishing this.\nNext, we will go through two ways that are commonly used to visualize the changing variance of returns. These are plotting the absolute value of price returns,\n\\[\\left| r^{SPY}_t \\right|, \\]\nor the square of price returns,\n\\[\\left( r^{SPY}_t \\right)^2.\\]\nBoth cases make sense since the variance is always a positive number, and influenced by deviations from the mean. This is only true, of course, if we know that the return series has mean 0, \\(r^{SPY}_t = 0 + \\epsilon_t\\). If this is true, then the average of squared returns is the sample variance,\n\\[\\hat{\\sigma}_t^2 = \\sum_1^{n} \\left( r^{SPY}_t \\right)^2.\\]\nIn price data, percent returns are almost have a mean very near to 0. If the mean return is non-zero, then we can just plot \\(\\left( r^{SPY}_t - \\mu\\right)^2\\), or use the squared errors from an ARIMA model. Since we did not find very strong ARMA effects in the previous post, and especially since the intercept (mean) was zero, we can get a good sense of daily variance of SPY price returns by simply plotting daily squared returns or daily absolute value of returns.\n\nlibrary(tibble)\nSPYRet <- add_column(SPYRet, SquaredReturns = SPYRet$value^2, AbsoluteReturns = abs(SPYRet$value))\n\nggplot(SPYRet, aes(x = index, y = AbsoluteReturns, color = series)) + \n  geom_line() + \n  theme_bw() +\n  labs(title = \"SPY Absolute Value of Returns from 2007 to 2017\", x = \"\")\n\n\n\nggplot(SPYRet, aes(x = index, y = SquaredReturns, color = series)) + \n  geom_line() + \n  theme_bw() +\n  labs(title = \"SPY Squared Returns from 2007 to 2017\", x = \"\")\n\n\n\n\n\nThe GARCH Model of Volatity\nThe plain vanilla (there are sooo many variations of the GARCH model) GARCH model is as follows:\n\\[\\begin{align}\nr_t &=\\mu+\\varepsilon_t  \\\\\n\\varepsilon_t &= \\sigma_t.z_t \\\\\n\\sigma_t^2 &=\\omega + \\alpha_1\\sigma_{t-1}^2 + \\beta_1\\varepsilon_{t-1}^2\\\\\nz_t &\\sim \\mathcal{N}(0,1).\n\\end{align}\\]\nThe first line is an equation to model the mean. As presented here there are no ARMA effects, but they could easily be thrown in if you find they are important. There is only an intercept and an error term. The next three lines put more structure on the error term, but it can be confusing what is going on here.\nI find the second line particularly confusing. Why do we multiply two things to get \\(\\epsilon_t\\) rather than add like you would normally see in a regression error term?\nTo see this, it is important to keep the goal in mind here. We are looking for a model that will give us a changing variance of \\(r^{SPY}_t\\). Namely, to find a model for \\(r^{SPY}_t\\) that has the following basic form:\n\\[r^{SPY}_t \\sim \\mathcal{N}(\\mu, \\sigma_t^2)\\]\nSo if the basic return model is \\(r^{SPY}_t =\\mu+\\varepsilon_t\\), it better be the case that \\(Var(\\mu+\\varepsilon_t) = \\sigma_t^2\\).\nThe next steps rely on the properties of the variance of random variables. Specifically, if \\(a\\) and \\(b\\) are a constants and \\(X\\) is a random variable, \\(Var(a + bX) = b^2Var(X)\\).\n\\[\\begin{align}\nVar(r^{SPY}_t) &= Var(\\mu+\\varepsilon_t) \\\\\n               &= Var(\\varepsilon_t)\n\\end{align}\\]\nSo if we come up with a model for \\(\\varepsilon_t\\) so that it’s variance depends on recent volatility and is big when recent volatility is big and is small when recent volatility is small, we will have created a model of conditional heteroskedasticity.\nConsider the second line in the GARCH model.\n\\[\\varepsilon_t = \\sigma_t.z_t\\]\nNotice that \\(\\sigma_t\\) is a constant, it is just a linear combination of past \\(\\sigma^2\\)’s and past \\(\\epsilon^2\\)’s, so it is known at time t.\n\\[\\begin{align}\nVar(r^{SPY}_t) &= Var(\\mu+\\varepsilon_t) \\\\\n               &= Var(\\varepsilon_t) \\text{      since } \\mu \\text{ is a constant}\\\\\n               &=Var(\\sigma_t, z_t) \\\\\n               &=\\sigma_t^2Var(z_t) \\text{       since } \\sigma^2_t \\text{ is a constant}\\\\\n               &=\\sigma_t^2\n\\end{align}\\]\nThe last line follows since \\(z_t \\sim N(0, 1)\\), and the \\(Var(z_t) = 1\\). So in the second equation of the GARCH model, multiplying the \\(\\sigma_t\\) and the \\(\\epsilon_t\\) takes advantage of the properties of variance to get just what we wanted, conditional variance of \\(r^{SPY}_t\\) that will be big when recent volatility is big and small when recent volatility is small. That last part follows because of how the \\(\\sigma^2_t\\) is consructed in the third line of the GARCH model, \\(\\sigma_t^2 =\\omega + \\alpha_1\\sigma_{t-1}^2 + \\beta_1\\varepsilon_{t-1}^2\\).\n\n\nEstimating a GARCH Model\nThe code below uses the rugarch R package to estimate a GARCH(p = 1, q = 1) model. Note that the p and q denote the number of lags on the \\(\\sigma^2_t\\) and \\(\\epsilon^2_t\\) terms, respectively.\nThe first command asks it to specify a plain vanilla GARCH by model = \"sGARCH\". It asks it to use an ARMA(1, 1) for the returns model by armaOrder = c(1, 1), include.mean = TRUE. We ask it to use the \\(\\mathcal{N}(0, 1)\\) distribution for the \\(z_t\\)’s with the distribution.model = \"norm\". The second command asks it to fit the model. The output is printed below the code.\nThe main model output is displayed under ‘Optimal Parameters’. The mu, ar1 and ma1 coefficients are from the mean model (ARMA(1, 1)). and the omega, alpha1, and beta1 are coefficient estimates from the \\(\\sigma_t^2 =\\omega + \\alpha_1\\sigma_{t-1}^2 + \\beta_1\\varepsilon_{t-1}^2\\) equation of the main GARCH model. Under ‘Robust Standard Errors’ are the same coefficient estimates, but the standard errors are widened to account for the possibility that our distributional assumption is wrong. Notice that the estimates are about the same, but the p-values are all bigger, and in the case of alpha1 it becomes no longer statistically significant with the robust standard errors.\n\nlibrary(rugarch)\n\ngarch11        <- ugarchspec(variance.model = list(model = \"sGARCH\", garchOrder = c(1, 1)), \n                            mean.model = list(armaOrder = c(1, 1), include.mean = TRUE), \n                            distribution.model = \"norm\")\n\ngarchfit       <- ugarchfit(spec = garch11, data = SPYRet_xts[\"2007-02-01/\"], solver = \"hybrid\")\ngarchfit\n\n\n*---------------------------------*\n*          GARCH Model Fit        *\n*---------------------------------*\n\nConditional Variance Dynamics   \n-----------------------------------\nGARCH Model : sGARCH(1,1)\nMean Model  : ARFIMA(1,0,1)\nDistribution    : norm \n\nOptimal Parameters\n------------------------------------\n        Estimate  Std. Error     t value Pr(>|t|)\nmu      0.000726    0.000153    4.755238 0.000002\nar1     0.936693    0.006192  151.274139 0.000000\nma1    -0.962207    0.001802 -534.051959 0.000000\nomega   0.000000    0.000005    0.020075 0.983984\nalpha1  0.103081    0.092193    1.118101 0.263524\nbeta1   0.909234    0.075771   11.999714 0.000000\n\nRobust Standard Errors:\n        Estimate  Std. Error   t value Pr(>|t|)\nmu      0.000726    0.019020  0.038189 0.969537\nar1     0.936693    0.307717  3.044003 0.002335\nma1    -0.962207    0.224501 -4.285978 0.000018\nomega   0.000000    0.000705  0.000137 0.999891\nalpha1  0.103081   13.472230  0.007651 0.993895\nbeta1   0.909234   11.079444  0.082065 0.934595\n\nLogLikelihood : 12972.04 \n\nInformation Criteria\n------------------------------------\n                    \nAkaike       -6.4077\nBayes        -6.3984\nShibata      -6.4077\nHannan-Quinn -6.4044\n\nWeighted Ljung-Box Test on Standardized Residuals\n------------------------------------\n                        statistic p-value\nLag[1]                      1.096  0.2952\nLag[2*(p+q)+(p+q)-1][5]     2.585  0.7318\nLag[4*(p+q)+(p+q)-1][9]     5.364  0.3745\nd.o.f=2\nH0 : No serial correlation\n\nWeighted Ljung-Box Test on Standardized Squared Residuals\n------------------------------------\n                        statistic  p-value\nLag[1]                      3.336 0.067790\nLag[2*(p+q)+(p+q)-1][5]    11.377 0.004054\nLag[4*(p+q)+(p+q)-1][9]    13.543 0.007969\nd.o.f=2\n\nWeighted ARCH LM Tests\n------------------------------------\n            Statistic Shape Scale P-Value\nARCH Lag[3]    0.4719 0.500 2.000  0.4921\nARCH Lag[5]    3.9631 1.440 1.667  0.1773\nARCH Lag[7]    4.4333 2.315 1.543  0.2884\n\nNyblom stability test\n------------------------------------\nJoint Statistic:  315.813\nIndividual Statistics:               \nmu      0.09393\nar1     0.07295\nma1     0.12386\nomega  41.18861\nalpha1  0.04252\nbeta1   0.02114\n\nAsymptotic Critical Values (10% 5% 1%)\nJoint Statistic:         1.49 1.68 2.12\nIndividual Statistic:    0.35 0.47 0.75\n\nSign Bias Test\n------------------------------------\n                   t-value      prob sig\nSign Bias            4.089 4.420e-05 ***\nNegative Sign Bias   1.434 1.516e-01    \nPositive Sign Bias   1.145 2.522e-01    \nJoint Effect        30.689 9.886e-07 ***\n\n\nAdjusted Pearson Goodness-of-Fit Test:\n------------------------------------\n  group statistic p-value(g-1)\n1    20     203.2    7.697e-33\n2    30     249.2    7.340e-37\n3    40     265.7    1.589e-35\n4    50     300.1    8.760e-38\n\n\nElapsed time : 3.790599 \n\n\nNow let’s use the rugarch’s standard functionality to use this estimated model to produce rolling forecasts of \\(\\sigma_t\\) and plot them versus \\(\\left| r^{SPY}_t \\right|\\).\n\nspec           <- getspec(garchfit)\nsetfixed(spec) <- as.list(coef(garchfit))\ngarchforecast1 <- ugarchforecast(spec, n.ahead = 1, n.roll = 2499, data = SPYRet_xts[\"2007-02-01/\"], out.sample = 2500)\n\nplot(garchforecast1, which = 4)\n\n\n\n\nIt looks like to me this model does a pretty good job of sensing how long a volatility spike will remain elevated, or rather modeling the path of a volatility spike back down to long-run mean levels. Since all econometric models use past values to predict current values, it cannot foresee the initial spike up in volatility.\n\n\nThat’s It!\nNow you know the GARCH model for forecasting volatility. In the next post we will cover the multi-equation models, VAR and VECM, that are good for estimating the relationship among many variables."
  },
  {
    "objectID": "tsbootcamp/2018-02-05-Basic-Time-Series-Analysis-VECM.html",
    "href": "tsbootcamp/2018-02-05-Basic-Time-Series-Analysis-VECM.html",
    "title": "Basic Time-Series Analysis: A Drunk and Her Dog Explain Cointegration and the VECM Model",
    "section": "",
    "text": "This post is the fifth in a series explaining Basic Time Series Analysis. Click the link to check out the first post which focused on stationarity versus non-stationarity, and to find a list of other topics covered. As a reminder, this post is intended to be a very applied example of how use certain tests and models in a time-sereis analysis, either to get someone started learning about time-series techniques or to provide a big-picture perspective to someone taking a formal time-series class where the stats are coming fast and furious. As in the first post, the code producing these examples is provided for those who want to follow along in R. If you aren’t into R, just ignore the code blocks and the intuition will follow.\nMy colleague @TKuethe showed me this awesome old paper by Michael Murray, A Drunk and Her Dog: An Illusration of Cointegration and Error Correction.1 Then I found this extension by Aaron Smith and Robin Harrison, A Drunk, Her Dog, and Boyfiend: An Illusration of Multiple Cointegration and Error Correction. The premise of both these papers is that if you follow a drunk out of the bar after a night of drinking, you will observe that her path looks much like a random walk. Also, if you have ever watched a dog freely exploring, it’s path also looks much like a random walk. The dog will go this way and that, wherever it’s nose leads it. If additionally, the dog belongs to the drunk, and the drunk calls periodically for the dog, the dog will stay pretty close to the drunk. So the drunk and her dog go forth wandering aimlessly, together.\nThis is exactly concept of cointegration, two (or more) series wandering aimlessly, together.\nIn this blog post we cover cointeration and most common model for cointegrated time-series, the Vector Error Correction Model (VECM). The most intuitive cases (besides the drunk and her dog) are markets that are related by a production process, like the soybean complex. Soybean crushers buy soybeans and sell meal and oil. Economic theory would suggest the prices of those three commodities to maintain a relationship so that the profitability of soybean crushers trends around some modest number greater than zero. If there were persistently high profits in the soybean crushing business, more soybean crushing capacity would be built, driving soybean prices higher and meal and oil prices lower from the increased demand (for soybeans) and supply (of meal and oil).\nThis can also come up in the stock market, especially if two companies are destined for similar fates. If you’ve ever read a trading book, trading this situation is sometimes called pairs trading. In commodities, it’s sometimes called spread trading. The idea is you look for two stocks (or commodity contracts) that are cointegrated (usually move together). Then you wait for a day when one goes up and one goes down. You bet that because they are cointegrated this deviation won’t last long, so you sell the one whose price went up and buy the one whose price went down. Then, no matter if the prices go up or down generally, as long as the two prices come back in line, you can make money.\nIn explaining the model, I couldn’t avoid using linear algebra this time, since the Johansen test itself calculates the determinant of a matrix.\nWe will go through specific examples of statistical tests for cointegration and some of the insights you can gain from fitting a VECM model of cointegrated series.\nIn this example we will use SPY (the S&P 500 exchange traded fund) and SHY (iShares 1-3 year Treasury Bond) prices. We don’t use Goldman Sachs prices in this example (in contrast to the previous posts in this series) because it turns out SPY and GS are not cointegrated, so I found a different example that is."
  },
  {
    "objectID": "tsbootcamp/2018-02-05-Basic-Time-Series-Analysis-VECM.html#the-johansen-test-for-cointegration",
    "href": "tsbootcamp/2018-02-05-Basic-Time-Series-Analysis-VECM.html#the-johansen-test-for-cointegration",
    "title": "Basic Time-Series Analysis: A Drunk and Her Dog Explain Cointegration and the VECM Model",
    "section": "The Johansen Test for Cointegration",
    "text": "The Johansen Test for Cointegration\nAlright, so that is a lot of nitty gritty matrix algebra, that I’ve been purposefully trying to avoid in this series of blog posts. But, the payoff is that we can talk about how the statistical test for cointegration works in a way that is analogous to t-tests that are more familiar.\nRecall that cointegration is when two or more variables are known to be non-stationary, yet a linear combination of these exist so that the linear combination is stationary. Notice that the error correction term uses SPY and SHY prices in levels. We know that we cannot put non-stationary variables in a linear regression, yet if SPY and SHY are cointegrated these two regression equations will be ok. Why? Because when you multiply the matrix of \\(\\beta\\)s times \\([1 SPY_{t-1} SHY_{t-1}]'\\) vector it will be stationary because the regression will find the \\(\\beta\\)’s (as long as such a matrix exists) that makes \\(\\beta_0 + \\beta_1 SPY_{t-1} + \\beta_2 SHY_{t-1}\\) stationary.\nNow if we don’t know whether or not our variables are cointegrated, we will need a test to determine this. The most common is called the Johansen cointegration test. What the Johansen test does is estimates the VECM and determines if the \\(\\alpha \\beta\\) matrix is ‘zero’. I put zero in quotes because the analogous concept of zero in numbers is the determinant in matrices. In a typical linear regression, if you are unsure whether you should include a variable on the right hand side you might just run the model and do a t-test to see if the coefficient on that variable is statistically different from zero.\nThe Johansen cointegration test does something similar. It estimates the VECM model and tests whether or not the determinant of the \\(\\alpha \\beta\\) matrix is zero. If it is not zero, then you have cointegration (probably - I’ll explain more in a bit)."
  },
  {
    "objectID": "tsbootcamp/2018-02-05-Basic-Time-Series-Analysis-VECM.html#apply-the-johansen-test",
    "href": "tsbootcamp/2018-02-05-Basic-Time-Series-Analysis-VECM.html#apply-the-johansen-test",
    "title": "Basic Time-Series Analysis: A Drunk and Her Dog Explain Cointegration and the VECM Model",
    "section": "Apply the Johansen Test",
    "text": "Apply the Johansen Test\nI think its really easiest to understand what the previous discussion means by looking at some actual output from a Johansen test.\n\nlibrary(urca)\njohansentest <- ca.jo(time_series, type = \"trace\", ecdet = \"const\", K = 3)\nsummary(johansentest)\n\n\n###################### \n# Johansen-Procedure # \n###################### \n\nTest type: trace statistic , without linear trend and constant in cointegration \n\nEigenvalues (lambda):\n[1] 1.071108e-02 2.535654e-03 3.938729e-18\n\nValues of teststatistic and critical values of test:\n\n          test 10pct  5pct  1pct\nr <= 1 | 10.32  7.52  9.24 12.97\nr = 0  | 54.08 17.85 19.96 24.60\n\nEigenvectors, normalised to first column:\n(These are the cointegration relations)\n\n             SPY.l3    SHY.l3  constant\nSPY.l3       1.0000    1.0000   1.00000\nSHY.l3     115.3853  -32.1019 -12.04965\nconstant -9711.8640 2233.5646 777.66905\n\nWeights W:\n(This is the loading matrix)\n\n             SPY.l3        SHY.l3     constant\nSPY.d  7.699219e-06 -1.271609e-03 2.839169e-18\nSHY.d -8.297597e-06  3.024787e-06 5.484280e-19\n\n\nAbove is the output from the Johansen trace test for cointegration. There is also a similar test called the eigen test, which I won’t cover in this post, but you interpret them the same way. They usually give the same conclusion, but not always.\nFocus on the output that has \\(r <= 1\\) and \\(r <= 0\\). These are the results from the actual Johansen test. The \\(r = 0\\) line is testing the hypothesis that the rank of the matrix equals zero (think testing if the matrix is ‘=’ zero, i.e., the error correction term doesn’t belong). Notice that the test statistic, 71.46, is quite a bit above the one percent critical value of 24.60. Hence we should reject the null hypothesis.\nNext we consider the hypothesis that \\(r <= 1\\), (which now that we have rejected \\(r = 0\\), the test is for \\(r = 1\\) since rank of a matrix is an integer). In this case, the test statistic is 8.19, which is less than all but the 10% critical values. Hence we fail to reject the null hypothesis and conclude the rank = 1.\nThis means that there is indeed one cointegrating relationship, and the VECM is appropriate.\nWhat if we had rejected the \\(r = 1\\) hypothesis? Then we would have to conclude the rank = 2. This would indicate the series are stationary after all."
  },
  {
    "objectID": "tsbootcamp/2018-02-05-Basic-Time-Series-Analysis-VECM.html#fit-the-vecm",
    "href": "tsbootcamp/2018-02-05-Basic-Time-Series-Analysis-VECM.html#fit-the-vecm",
    "title": "Basic Time-Series Analysis: A Drunk and Her Dog Explain Cointegration and the VECM Model",
    "section": "Fit the VECM",
    "text": "Fit the VECM\nThe fitted VECM is actually delivered from the call to ca.jo, but it is put into a bit more intuitive form by passing the johansentest object to the cajorls() function.\n\nt <- cajorls(johansentest, r = 1)\nt\n\n$rlm\n\nCall:\nlm(formula = substitute(form1), data = data.mat)\n\nCoefficients:\n         SPY.d       SHY.d     \nect1      7.699e-06  -8.298e-06\nSPY.dl1  -8.251e-02   5.513e-04\nSHY.dl1   1.869e+00  -7.247e-02\nSPY.dl2   2.974e-02  -5.027e-04\nSHY.dl2   3.700e-01  -5.681e-02\n\n\n$beta\n               ect1\nSPY.l3       1.0000\nSHY.l3     115.3853\nconstant -9711.8640\n\n\nIn the ‘Coefficients’ output, the \\(\\alpha\\)’s and \\(\\gamma\\)’s are given. Specifically, \\(\\alpha_1\\), the coeficient on the error correction term in the SPY equation, is 1.319e-06, and \\(\\alpha_2\\) is 9.070e-06. The remaining are the \\(\\gamma\\)’s, the coefficients on the lagged return variables.\nThe ‘\\(\\beta\\)’ output gives the error correction term. Here we have that \\(\\beta_0\\) is 16200.3089, \\(\\beta_1\\) is 1, and \\(\\beta_2\\) is -193.8095.\nNotice that information is also present in the previous output. The \\(\\alpha\\)’s are the first column of the ‘loading matrix’, and the \\(\\beta\\)’s are the first column of the eigenvectors matrix."
  },
  {
    "objectID": "tsbootcamp/2018-02-05-Basic-Time-Series-Analysis-VECM.html#what-does-this-tell-you",
    "href": "tsbootcamp/2018-02-05-Basic-Time-Series-Analysis-VECM.html#what-does-this-tell-you",
    "title": "Basic Time-Series Analysis: A Drunk and Her Dog Explain Cointegration and the VECM Model",
    "section": "What does this tell you?",
    "text": "What does this tell you?\nIn my opinion, this model provides more opportunity for ‘economic analysis’ than the previous models we covered in this series. The \\(\\beta\\)’s tell you what the long run equilibrium among the series looks like, which should be influenced by various economic factors. For example, in this paper my coauthors and I showed that a VECM model on one year deferred corn, ethanol, and natural gas prices produces an error correction term that looks an awful lot like what you would get if you calculate a no-profit condition in the ethanol industry.\nFinally, as we mentioned before, the \\(\\alpha\\)’s tell you how fast the series tend to move back together when they get out of whack, which speaks to the nature of frictions in the markets that might allow departures from long-run equilibrium."
  },
  {
    "objectID": "tsbootcamp/2018-02-05-Basic-Time-Series-Analysis-VECM.html#granger-causality-on-cointegrated-series",
    "href": "tsbootcamp/2018-02-05-Basic-Time-Series-Analysis-VECM.html#granger-causality-on-cointegrated-series",
    "title": "Basic Time-Series Analysis: A Drunk and Her Dog Explain Cointegration and the VECM Model",
    "section": "Granger Causality on Cointegrated Series",
    "text": "Granger Causality on Cointegrated Series\nYou can also do Granger causality analysis on cointegrated series, but be warned you should take care in how you execute it. See Dave Giles’ excellent post about it, and this post of how to implement the test in R."
  },
  {
    "objectID": "tsbootcamp/2018-02-12-Basic_Time-Series-Cookbook.html",
    "href": "tsbootcamp/2018-02-12-Basic_Time-Series-Cookbook.html",
    "title": "Basic Time-Series Analysis: Model Choice Cookbook",
    "section": "",
    "text": "This post is the sixth and final in a series explaining Basic Time Series Analysis. Click the link to check out the first post which focused on stationarity versus non-stationarity, and to find a list of other topics covered. As a reminder, this post is intended to be a very applied example of how use certain tests and models in a time-sereis analysis, either to get someone started learning about time-series techniques or to provide a big-picture perspective to someone taking a formal time-series class where the stats are coming fast and furious. As in the first post, the code producing these examples is provided for those who want to follow along in R. If you aren’t into R, just ignore the code blocks and the intuition will follow.\nThis post concludes the sequence of posts on time series basics. It certainly doesn’t replace the need for a good course in time-series econometrics/statistics; there are a lot of complicating factors and model extensions I didn’t touch upon. But, I hope beginners can use it to get started poking at their own questions, or reading others’ research based on little more than what is covered in these posts.\nSo with that, the final post in this series provides a ‘cookbook’ approach, or series of questions along a decision tree to guide you toward what kind of model you should be thinking about for your own analysis."
  },
  {
    "objectID": "tsbootcamp/2018-02-12-Basic_Time-Series-Cookbook.html#blogs",
    "href": "tsbootcamp/2018-02-12-Basic_Time-Series-Cookbook.html#blogs",
    "title": "Basic Time-Series Analysis: Model Choice Cookbook",
    "section": "Blogs",
    "text": "Blogs\nThere are tons of great stats and time series blogs out there. Forgive me if I don’t link to your favorite (or the one you write!) But, there are a couple that stand out in my mind so I will link them here as resources to learn more. If I’ve overlooked any obvious gems, please post in the comments!\nDave Giles’ Econometrics Beat: This blog is one of the best and most widely read on the web. Dave gets down into the nitty gritty, much more so than I have done in this series. I learn something every time I take the time to read one of his posts. He doesn’t cover time series exclusively, but I believe the majority of posts are about time series topics.\nRob Hyndman’s Hyndsight Blog: In the blog itself, Rob doesn’t get into explaining econometrics too much. However, he has been one of the most prolific creators or R packages for time series and forecasting. His website is a good gateway to all of that material."
  },
  {
    "objectID": "tsbootcamp/2018-02-27-Trees-Forests-Boosting.html",
    "href": "tsbootcamp/2018-02-27-Trees-Forests-Boosting.html",
    "title": "Machine Learning and Econometrics: Trees, Random Forests, and Boosting",
    "section": "",
    "text": "This is the third in a series of posts where I document my own process in figuring out how machine learning relates to the classic econometrics one learns in a graduate program in economics. Note that I am a humble practitioner of applied econometrics, so the ideas I am working through are not new, and my first take on them might not even be fully correct. But, I think many of us applied economists are starting to learn and dabble in this area and I thought it might be fruitful to community if I learn publicly in these blog posts. These posts are certainly going to solidify my own understanding and serve as helpful notes for my own purposes.\nIn this post I summarize what I learned from Chapter 8 of Introduction to Statistical Learning (ISL). In my last post I covered cross validation from chapter 5. I am skipping chapters 6 (Linear Model Selection and Regularization) and 7 (Moving Beyond Linearity) partly because I have kind of seen some of that stuff before and partly because I need to move my learning along in order to get something prepared for the SCC-76 meeting April 5-7th (Todd and Todd, I hope you aren’t getting nervous yet!).\nI’m really excited to write up this post because it covers something that I truly know zero about, and feels like ‘real’ machine learning. Tree-Based Methods! I’ll briefly describe regression trees, bagging, random forests, and boosting."
  },
  {
    "objectID": "tsbootcamp/2018-02-27-Trees-Forests-Boosting.html#a-few-caveats-up-front",
    "href": "tsbootcamp/2018-02-27-Trees-Forests-Boosting.html#a-few-caveats-up-front",
    "title": "Machine Learning and Econometrics: Trees, Random Forests, and Boosting",
    "section": "A Few Caveats Up-Front",
    "text": "A Few Caveats Up-Front\nI am going to use data going back to 2007, but I am restricting my choices to the current top 10 market cap companies. Companies like Google and Amazon were not in the top 10 by market cap in 2007, so the fact that they ascended to the top 10 by today builds in a lot of look-back bias. If you know a company go from not being in the top 10, to being in the top 10 you don’t need a fancy forecasting model to make money. Just buy it.\nI could do this better if I restricted my decision set to who was in the top 10 at the time of the forecast. I could also do better if I set up my forecast to use a rolling window of, say, 50 or 100 observations and then re-estimate the boosted model each day. Both of those things would reduce the look back bias, but the purpose of this post is more for me to get a feel for the boosting algorithm than it is to find the best realistic forecasting model.\nAnd like I always tell my students, if I ever find a forecasting model that actually works well enough to make money, I’ll never tell you about it!"
  },
  {
    "objectID": "tsbootcamp/2018-02-27-Trees-Forests-Boosting.html#the-details",
    "href": "tsbootcamp/2018-02-27-Trees-Forests-Boosting.html#the-details",
    "title": "Machine Learning and Econometrics: Trees, Random Forests, and Boosting",
    "section": "The Details",
    "text": "The Details\nThe code below loads the required R packages and downloads the stocks that I want.\n\n# If you haven't installed any of the packages listed below, do so with the \"install.packages('tibble')\" command. \nlibrary(tibble)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(knitr)\nlibrary(quantmod)\nlibrary(randomForest)\nlibrary(PerformanceAnalytics)\nlibrary(gbm)\nlibrary(broom)\n\n#SPY and Top 10 S&P 500 Companies by Market Cap (BRK.B replaced by #11 BAC, bc Berkshire wouldn't download. FB replaced by #12 WFC bc I want long sample and FB has only been trading since '12. Plus 4 of the biggest Spyder sector etfs.) XLF - Financials, XLK - Tech, XLE - Energy, XLY - Consumer Discretionary\nSP  <- c(\"SPY\", \"AAPL\", \"MSFT\", \"AMZN\", \"WFC\", \"JPM\", \"BAC\", \"JNJ\", \"GOOG\", \"XOM\", \"XLF\", \"XLK\", \"XLE\", \"XLY\")\ngetSymbols(SP)\n\n [1] \"SPY\"  \"AAPL\" \"MSFT\" \"AMZN\" \"WFC\"  \"JPM\"  \"BAC\"  \"JNJ\"  \"GOOG\" \"XOM\" \n[11] \"XLF\"  \"XLK\"  \"XLE\"  \"XLY\" \n\nSP <- cbind(SPY$SPY.Adjusted, AAPL$AAPL.Adjusted, MSFT$MSFT.Adjusted, AMZN$AMZN.Adjusted, WFC$WFC.Adjusted, JPM$JPM.Adjusted, BAC$BAC.Adjusted, JNJ$JNJ.Adjusted, GOOG$GOOG.Adjusted, XOM$XOM.Adjusted, XLF$XLF.Adjusted, XLK$XLK.Adjusted, XLE$XLE.Adjusted, XLY$XLY.Adjusted)\ncolnames(SP) <- c(\"SPY\", \"AAPL\", \"MSFT\", \"AMZN\", \"WFC\", \"JPM\", \"BAC\", \"JNJ\", \"GOOG\", \"XOM\", \"XLF\", \"XLK\", \"XLE\", \"XLY\")\n\nThe next chunk of code calculates percentage returns, create lags, and combine into one object, SPRet.\n\nSPRet <- Return.calculate(SP, method = 'log')\nlaggs.1 <- apply(SPRet, 2, Lag, 1)\nlaggs.2 <- apply(SPRet, 2, Lag, 2)\nlaggs.3 <- apply(SPRet, 2, Lag, 3)\nlaggs.4 <- apply(SPRet, 2, Lag, 4)\nlaggs.5 <- apply(SPRet, 2, Lag, 5)\nlaggs.6 <- apply(SPRet, 2, Lag, 6)\nlaggs.7 <- apply(SPRet, 2, Lag, 7)\nlaggs.8 <- apply(SPRet, 2, Lag, 8)\nlaggs.9 <- apply(SPRet, 2, Lag, 9)\nlaggs.10 <- apply(SPRet, 2, Lag, 10)\nlaggs   <- cbind(laggs.1, laggs.2, laggs.3, laggs.4, laggs.5, laggs.6, laggs.7, laggs.8, laggs.9, laggs.10)\n\nSPRet   <- cbind(SPRet, laggs)\nNAMES   <- c(\"SPY\", \"AAPL\", \"MSFT\", \"AMZN\", \"WFC\", \"JPM\", \"BAC\", \"JNJ\", \"GOOG\", \"XOM\", \"XLF\", \"XLK\", \"XLE\", \"XLY\")\n\nIn the next code chunk we split the data into training and testing sub-samples. Then it uses the gbm() function from the gbm package to use a gradient boosting machine to implement the boosted regression trees. For a more technical description of the gbm, see here. We we get boosted predictions for each of the stocks we are considering.\n\n#set.seed(15) #GOOD\n#set.seed(12) #also good \n#train         <- sample(12:nrow(SPRet[12:dim(SPRet)[1]]), nrow(SPRet[12:dim(SPRet)[1]])/1.5)\n#temp          <- SPRet[train]\n\ntemp          <- SPRet[12:2110,]\ntesting       <- SPRet[2110:(dim(SPRet)[1]),]\nntrees        <- 1000\ndepth         <- 1\n\n# SPY\nSPRtemp       <- temp[, setdiff(colnames(SPRet), c(\"AAPL\", \"MSFT\", \"AMZN\", \"WFC\", \"JPM\", \"BAC\", \"JNJ\", \"GOOG\", \"XOM\", \"XLF\", \"XLK\", \"XLE\", \"XLY\"))]\n\nboost.SPY     <- gbm(SPY~., data= SPRtemp, distribution = \"gaussian\", n.trees = ntrees, interaction.depth = 4, shrinkage = .001)\nyhat.SPY      <- predict(boost.SPY, newdata = testing, n.trees = ntrees)\nSPRtempFull   <- SPRet[2110:(dim(SPRet)[1]), setdiff(colnames(SPRet), c(\"AAPL\", \"MSFT\", \"AMZN\", \"WFC\", \"JPM\", \"BAC\", \"JNJ\", \"GOOG\", \"XOM\", \"XLF\", \"XLK\", \"XLE\", \"XLY\"))]\nyhat.SPY      <- predict(boost.SPY, newdata = SPRtempFull, n.trees = ntrees)\nindicator.SPY <- apply(as.matrix(Lag(yhat.SPY)[2:dim(Lag(yhat.SPY))]), 1, function(x) if(x<0) (-1) else 1)\n\n\n# AAPL\nSPRtemp       <- temp[, setdiff(colnames(SPRet), c(\"SPY\", \"MSFT\", \"AMZN\", \"WFC\", \"JPM\", \"BAC\", \"JNJ\", \"GOOG\", \"XOM\", \"XLF\", \"XLK\", \"XLE\", \"XLY\"))]\n\nboost.AAPL     <- gbm(AAPL~., data= SPRtemp, distribution = \"gaussian\", n.trees = ntrees, interaction.depth = depth, shrinkage = .001)\nyhat.AAPL      <- predict(boost.AAPL, newdata = testing, n.trees = ntrees)\nSPRtempFull    <- SPRet[2110:(dim(SPRet)[1]), setdiff(colnames(SPRet), c(\"SPY\", \"MSFT\", \"AMZN\", \"WFC\", \"JPM\", \"BAC\", \"JNJ\", \"GOOG\", \"XOM\", \"XLF\", \"XLK\", \"XLE\", \"XLY\"))]\nyhat.AAPL      <- predict(boost.AAPL, newdata = SPRtempFull, n.trees = ntrees)\nindicator.AAPL <- apply(as.matrix(Lag(yhat.AAPL)[2:dim(Lag(yhat.SPY))]), 1, function(x) if(x<0) (-1) else 1)\n\n# MSFT\nSPRtemp        <- temp[, setdiff(colnames(SPRet), c(\"SPY\", \"AAPL\", \"AMZN\", \"WFC\", \"JPM\", \"BAC\", \"JNJ\", \"GOOG\", \"XOM\", \"XLF\", \"XLK\", \"XLE\", \"XLY\"))]\n\nboost.MSFT     <- gbm(MSFT~., data= SPRtemp, distribution = \"gaussian\", n.trees = ntrees, interaction.depth = depth, shrinkage = .001)\nyhat.MSFT      <- predict(boost.MSFT, newdata = testing, n.trees = ntrees)\nSPRtempFull    <- SPRet[2110:(dim(SPRet)[1]), setdiff(colnames(SPRet), c(\"SPY\", \"AAPL\", \"AMZN\", \"WFC\", \"JPM\", \"BAC\", \"JNJ\", \"GOOG\", \"XOM\", \"XLF\", \"XLK\", \"XLE\", \"XLY\"))]\nyhat.MSFT      <- predict(boost.MSFT, newdata = SPRtempFull, n.trees = ntrees)\nindicator.MSFT <- apply(as.matrix(Lag(yhat.MSFT)[2:dim(Lag(yhat.SPY))]), 1, function(x) if(x<0) (-1) else 1)\n\n# AMZN\nSPRtemp        <- temp[, setdiff(colnames(SPRet), c(\"SPY\", \"AAPL\", \"MSFT\", \"WFC\", \"JPM\", \"BAC\", \"JNJ\", \"GOOG\", \"XOM\", \"XLF\", \"XLK\", \"XLE\", \"XLY\"))]\n\nboost.AMZN     <- gbm(AMZN~., data= SPRtemp, distribution = \"gaussian\", n.trees = ntrees, interaction.depth = depth, shrinkage = .001)\nyhat.AMZN      <- predict(boost.AMZN, newdata = testing, n.trees = ntrees)\nSPRtempFull    <- SPRet[2110:(dim(SPRet)[1]), setdiff(colnames(SPRet), c(\"SPY\", \"AAPL\", \"MSFT\", \"WFC\", \"JPM\", \"BAC\", \"JNJ\", \"GOOG\", \"XOM\", \"XLF\", \"XLK\", \"XLE\", \"XLY\"))]\nyhat.AMZN      <- predict(boost.AMZN, newdata = SPRtempFull, n.trees = ntrees)\nindicator.AMZN <- apply(as.matrix(Lag(yhat.AMZN)[2:dim(Lag(yhat.SPY))]), 1, function(x) if(x<0) (-1) else 1)\n\n# WFC\nSPRtemp        <- temp[, setdiff(colnames(SPRet), c(\"SPY\", \"AAPL\", \"MSFT\", \"AMZN\", \"JPM\", \"BAC\", \"JNJ\", \"GOOG\", \"XOM\", \"XLF\", \"XLK\", \"XLE\", \"XLY\"))]\n\nboost.WFC     <- gbm(WFC~., data= SPRtemp, distribution = \"gaussian\", n.trees = ntrees, interaction.depth = depth, shrinkage = .001)\nyhat.WFC      <- predict(boost.WFC, newdata = testing, n.trees = ntrees)\nSPRtempFull    <- SPRet[2110:(dim(SPRet)[1]), setdiff(colnames(SPRet), c(\"SPY\", \"AAPL\", \"MSFT\", \"AMZN\", \"JPM\", \"BAC\", \"JNJ\", \"GOOG\", \"XOM\", \"XLF\", \"XLK\", \"XLE\", \"XLY\"))]\nyhat.WFC      <- predict(boost.WFC, newdata = SPRtempFull, n.trees = ntrees)\nindicator.WFC <- apply(as.matrix(Lag(yhat.WFC)[2:dim(Lag(yhat.SPY))]), 1, function(x) if(x<0) (-1) else 1)\n\n# JPM\nSPRtemp        <- temp[, setdiff(colnames(SPRet), c(\"SPY\", \"AAPL\", \"MSFT\", \"AMZN\", \"WFC\", \"BAC\", \"JNJ\", \"GOOG\", \"XOM\", \"XLF\", \"XLK\", \"XLE\", \"XLY\"))]\n\nboost.JPM     <- gbm(JPM~., data= SPRtemp, distribution = \"gaussian\", n.trees = ntrees, interaction.depth = depth, shrinkage = .001)\nyhat.JPM      <- predict(boost.JPM, newdata = testing, n.trees = ntrees)\nSPRtempFull    <- SPRet[2110:(dim(SPRet)[1]), setdiff(colnames(SPRet), c(\"SPY\", \"AAPL\", \"MSFT\", \"AMZN\", \"WFC\", \"BAC\", \"JNJ\", \"GOOG\", \"XOM\", \"XLF\", \"XLK\", \"XLE\", \"XLY\"))]\nyhat.JPM      <- predict(boost.JPM, newdata = SPRtempFull, n.trees = ntrees)\nindicator.JPM <- apply(as.matrix(Lag(yhat.JPM)[2:dim(Lag(yhat.SPY))]), 1, function(x) if(x<0) (-1) else 1)\n\n# BAC\nSPRtemp        <- temp[, setdiff(colnames(SPRet), c(\"SPY\", \"AAPL\", \"MSFT\", \"AMZN\", \"WFC\", \"JPM\", \"JNJ\", \"GOOG\", \"XOM\", \"XLF\", \"XLK\", \"XLE\", \"XLY\"))]\n\nboost.BAC     <- gbm(BAC~., data= SPRtemp, distribution = \"gaussian\", n.trees = ntrees, interaction.depth = depth, shrinkage = .001)\nyhat.BAC      <- predict(boost.BAC, newdata = testing, n.trees = ntrees)\nSPRtempFull    <- SPRet[2110:(dim(SPRet)[1]), setdiff(colnames(SPRet), c(\"SPY\", \"AAPL\", \"MSFT\", \"AMZN\", \"WFC\", \"JPM\", \"JNJ\", \"GOOG\", \"XOM\", \"XLF\", \"XLK\", \"XLE\", \"XLY\"))]\nyhat.BAC      <- predict(boost.BAC, newdata = SPRtempFull, n.trees = ntrees)\nindicator.BAC <- apply(as.matrix(Lag(yhat.BAC)[2:dim(Lag(yhat.SPY))]), 1, function(x) if(x<0) (-1) else 1)\n\n# JNJ\nSPRtemp        <- temp[, setdiff(colnames(SPRet), c(\"SPY\", \"AAPL\", \"MSFT\", \"AMZN\", \"WFC\", \"JPM\", \"BAC\", \"GOOG\", \"XOM\", \"XLF\", \"XLK\", \"XLE\", \"XLY\"))]\n\nboost.JNJ     <- gbm(JNJ~., data= SPRtemp, distribution = \"gaussian\", n.trees = ntrees, interaction.depth = depth, shrinkage = .001)\nyhat.JNJ      <- predict(boost.JNJ, newdata = testing, n.trees = ntrees)\nSPRtempFull    <- SPRet[2110:(dim(SPRet)[1]), setdiff(colnames(SPRet), c(\"SPY\", \"AAPL\", \"MSFT\", \"AMZN\", \"WFC\", \"JPM\", \"BAC\", \"GOOG\", \"XOM\", \"XLF\", \"XLK\", \"XLE\", \"XLY\"))]\nyhat.JNJ      <- predict(boost.JNJ, newdata = SPRtempFull, n.trees = ntrees)\nindicator.JNJ <- apply(as.matrix(Lag(yhat.JNJ)[2:dim(Lag(yhat.SPY))]), 1, function(x) if(x<0) (-1) else 1)\n\n# GOOG\nSPRtemp        <- temp[, setdiff(colnames(SPRet), c(\"SPY\", \"AAPL\", \"MSFT\", \"AMZN\", \"WFC\", \"JPM\", \"BAC\", \"JNJ\", \"XOM\", \"XLF\", \"XLK\", \"XLE\", \"XLY\"))]\n\nboost.GOOG     <- gbm(GOOG~., data= SPRtemp, distribution = \"gaussian\", n.trees = ntrees, interaction.depth = depth, shrinkage = .001)\nyhat.GOOG      <- predict(boost.GOOG, newdata = testing, n.trees = ntrees)\nSPRtempFull    <- SPRet[2110:(dim(SPRet)[1]), setdiff(colnames(SPRet), c(\"SPY\", \"AAPL\", \"MSFT\", \"AMZN\", \"WFC\", \"JPM\", \"BAC\", \"JNJ\", \"XOM\", \"XLF\", \"XLK\", \"XLE\", \"XLY\"))]\nyhat.GOOG      <- predict(boost.GOOG, newdata = SPRtempFull, n.trees = ntrees)\nindicator.GOOG <- apply(as.matrix(Lag(yhat.GOOG)[2:dim(Lag(yhat.SPY))]), 1, function(x) if(x<0) (-1) else 1)\n\nSPRtempFull    <- SPRet[2110:(dim(SPRet)[1]),]\n\nNow, the next chunk takes all the predictions, and each day records the stock that has the highest return in absolute value, and also records whether it was positive or negative so we know whether to buy or sell.\n\n# Picking Stock with highest absolute value of predicted return. \nall            <- as.data.frame(cbind(yhat.SPY, yhat.AAPL, yhat.AMZN, yhat.MSFT, yhat.WFC, yhat.JPM, yhat.BAC, yhat.JNJ, yhat.GOOG))\ncolnames(all)  <- c(\"SPY\", \"AAPL\", \"AMZN\", \"MSFT\", \"WFC\", \"JPM\", \"BAC\", \"JNJ\", \"GOOG\")\n\nTemp           <- apply(abs(all), 1, which.max)\nreturn.picker  <- data.frame()\nfor (i in 1:length(Temp)){\nreturn.picker[i,1] <- all[i, Temp[i]]\n}\n\n# Long or Short\nindicator.picker <- apply(return.picker, 1, function(x) if(x<0) (-1) else 1)\n\n# Actual Returns of Picked Stocks\nreturn.Picker1  <- data.frame(stringsAsFactors = FALSE)\nreturn.Picker2  <- data.frame(stringsAsFactors = FALSE)\nreturn.Picker3  <- data.frame(stringsAsFactors = FALSE)\n\nfor (i in 1:length(Temp)){\nreturn.Picker1[i, 1] <- SPRtempFull[i, Temp[i]]\nreturn.Picker2[i, 1] <- colnames(all)[Temp[i]]\nreturn.Picker3[i, 1] <- indicator.picker[i]\n}\nreturn.Picker1    <- xts(return.Picker1 , order.by = index(SPRtempFull))\nreturn.Picker2    <- xts(return.Picker2 , order.by = index(SPRtempFull))\nreturn.Picker3    <- xts(return.Picker3 , order.by = index(SPRtempFull))\n\nOK, now the next chunk of code takes $10,000 and starts buying or selling every day in the test set, which is after about September 2014.\n\ninvest         <- 10000\n\nDATA            <- cbind(invest*cumprod(1+SPRtempFull$SPY), invest*cumprod(1+return.Picker1), invest*cumprod(1+SPRtempFull$AMZN), invest*cumprod(1+SPRtempFull$AAPL))\ncolnames(DATA)  <- c('SPY', \"Picker\", \"AMZN\", \"AAPL\") \n\nDATA            <- tidy(DATA)\n\nHow did the boosting stock picker do? I plot the cumulative returns of just buying and holding the SPY, Apple, and Amazon. Our boosted stock picker returns are in blue.\nThe boosting stock picker had us long Apple most of the time. Then in the test period, Amazon was the monster. But since we were not re-estimating the model, it never pivoted. I kind of expected it to recognize the large returns of Amazon and switch, even though it was using a dated model.\n\nggplot(DATA, aes(x = index, y = value, color = series)) + \n  geom_line() + \n  theme_bw() +\n  labs(title = \"Buy & Hold SPY, AAPL, AMZN vs 'Boosting' Picker, $10K Invested\", x = \"\")\n\n\n\n\nThis chart is just a count of the number of days in each stock.\n\nggplot(return.Picker2, aes(x = V1)) + geom_bar() + theme_bw()\n\n\n\n\nAnd this chart is a count of the number of days we were long (1) versus short (-1).\n\nggplot(return.Picker3, aes(x = V1)) + geom_bar() + theme_bw()\n\n\n\n\nThe interesting thing is, that this method of stock picking with a boosting algorithm on lagged returns basically gives you the same result any trend following strategy would. And those are generally much simpler. The are along the lines of buy if the price is above the 50 day moving average, etc. I guess it does independently suggest that trend following is better than contrarian strategies which for example might sell if the price is above the 50 day moving average.\nI’m curious about what the result would be in commodities. Stocks generally trend up, unless we are in a major correction. Commodities do not have that same property.\nPerhaps another day…"
  },
  {
    "objectID": "tsbootcamp.html",
    "href": "tsbootcamp.html",
    "title": "Time Series Bootcamp",
    "section": "",
    "text": "Basic Time-Series Analysis, the Game\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nBasic Time-Series Analysis: A Drunk and Her Dog Explain Cointegration and the VECM Model\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nBasic Time-Series Analysis: Model Choice Cookbook\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nBasic Time-Series Analysis: Modeling Volatility (GARCH)\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nBasic Time-Series Analysis: Single Equation Models (ARIMA)\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nBasic Time-Series Analysis: The VAR Model Explained?\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nMachine Learning and Econometrics: Trees, Random Forests, and Boosting\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  }
]